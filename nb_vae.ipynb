{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tfe = tf.contrib.eager\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.backend import batch_flatten\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "#import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "import pathlib\n",
    "AUTOTUNE=tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paths(directory):\n",
    "    dir=pathlib.Path.cwd()/directory\n",
    "    all_image_paths=list(dir.glob('*'))\n",
    "    return [str(path) for path in all_image_paths]\n",
    "\n",
    "\n",
    "def preprocess_image(image, size):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (size, size))\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    #image = tf.image.convert_image_dtype(image, tf.float16)\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path, size):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image, size)\n",
    "\n",
    "def from_path_to_tensor(paths, batch_size, size):\n",
    "    path_ds=tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds=path_ds.map(lambda x: load_and_preprocess_image(x, size), num_parallel_calls=1)\n",
    "    #ds=ds.repeat()\n",
    "    #ds=ds.shuffle(5000)\n",
    "    ds=ds.batch(batch_size)\n",
    "    ds=ds.prefetch(buffer_size=1)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got some model ideas from here https://medium.com/@jonathan_hui/gan-dcgan-deep-convolutional-generative-adversarial-networks-df855c438f\n",
    "# http://karpathy.github.io/2019/04/25/recipe/\n",
    "# https://towardsdatascience.com/deciding-optimal-filter-size-for-cnns-d6f7b56f9363\n",
    "\n",
    "# image dim must be divisible by 8\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, image_dim, mode, kernelsize=3, selected_layers = None, loader=None):\n",
    "        super(VAE, self).__init__()\n",
    "        if loader == None:\n",
    "            self.inference_net = tf.keras.Sequential(\n",
    "              [\n",
    "              tf.keras.layers.Conv2D(\n",
    "                  filters=8, kernel_size=3, strides=(2, 2), activation='relu', use_bias=False, input_shape=(image_dim, image_dim, 3)),\n",
    "              tf.keras.layers.BatchNormalization(),\n",
    "              tf.keras.layers.Conv2D(\n",
    "                  filters=4, kernel_size=3, strides=(2, 2), activation='relu', use_bias=False),\n",
    "              tf.keras.layers.BatchNormalization(),\n",
    "              tf.keras.layers.Conv2D(\n",
    "                  filters=2, kernel_size=3, strides=(2, 2), activation='relu', use_bias=False),\n",
    "              tf.keras.layers.BatchNormalization(),\n",
    "              tf.keras.layers.Flatten(),\n",
    "              # No activation\n",
    "              tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "              ]\n",
    "                )\n",
    "\n",
    "            self.generative_net = tf.keras.Sequential(\n",
    "            [\n",
    "              tf.keras.layers.Dense(units=24*24*32, activation=tf.nn.relu, input_shape=(latent_dim,)),\n",
    "              tf.keras.layers.Reshape(target_shape=(24, 24, 32)),\n",
    "              tf.keras.layers.Conv2DTranspose(\n",
    "                  filters=2,\n",
    "                  kernel_size=3,\n",
    "                  strides=(2, 2),\n",
    "                  padding=\"SAME\",\n",
    "                  activation='relu'),\n",
    "              tf.keras.layers.Conv2DTranspose(\n",
    "                  filters=4,\n",
    "                  kernel_size=3,\n",
    "                  strides=(2, 2),\n",
    "                  padding=\"SAME\",\n",
    "                  activation='relu'),\n",
    "              tf.keras.layers.Conv2DTranspose(\n",
    "                  filters=8,\n",
    "                  kernel_size=3,\n",
    "                  strides=(2, 2),\n",
    "                  padding=\"SAME\",\n",
    "                  activation='relu'),  \n",
    "              # No activation\n",
    "              tf.keras.layers.Conv2DTranspose(\n",
    "                  filters=3, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'),\n",
    "                        ]\n",
    "                    )\n",
    "        \n",
    "        if loader:\n",
    "            self.inference_net = tf.keras.models.load_model(loader+'/inf')\n",
    "            self.generative_net = tf.keras.models.load_model(loader+'/gen')\n",
    "        \n",
    "        if mode == 'dfc' or mode == 'combo':\n",
    "            self.percep_net = tf.keras.models.clone_model(self.inference_net)\n",
    "            self.percep_net.set_weights(self.inference_net.get_weights())\n",
    "\n",
    "\n",
    "        # if no layers are specififed, use the first two convolution layers\n",
    "        if selected_layers:\n",
    "            self.selected_layers = selected_layers\n",
    "        else:\n",
    "            self.selected_layers = [layer.name for layer in self.inference_net.layers if layer.name.startswith('conv')][:2]\n",
    "\n",
    "    @tf.function\n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    @tf.function\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=tf.shape(mean))\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    @tf.function\n",
    "    def decode(self, z):\n",
    "        return self.generative_net(z)\n",
    "\n",
    "    @tf.function\n",
    "    def get_features(self, x):\n",
    "        rv = []\n",
    "        for layer in self.percep_net.layers:\n",
    "                # We do not want to apply updates for the inference pass\n",
    "                x=layer(x)\n",
    "                if layer.name in self.selected_layers:\n",
    "                    rv.append(x)\n",
    "                if len(rv) == len(self.selected_layers):\n",
    "                    return rv\n",
    "\n",
    "    def saver(self, DIR, tag):\n",
    "        directory = './'+DIR+'/'+tag\n",
    "        if not os.path.exists(directory):\n",
    "            os.mkdir(directory)\n",
    "        self.inference_net.save(directory+'/inf', save_format='h5')\n",
    "        self.generative_net.save(directory+'/gen', save_format='h5')\n",
    "        \n",
    "    def loader(self, directory):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss definitions\n",
    "\n",
    "# Couldn't figure how to do some simple per pixel mse... smh tensorlfow!\n",
    "@tf.function\n",
    "def mse(label, prediction):\n",
    "    #flatten the tensors, maintaining batch dim\n",
    "    return tf.losses.MSE(batch_flatten(label), batch_flatten(prediction))\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(model, x, mode, scales, test=False):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_r = model.decode(z)\n",
    "    rv = {}\n",
    "\n",
    "    # Regularization term (KL divergence)\n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar), axis=-1)\n",
    "    if 'kl_loss' in scales.keys(): kl_loss *= scales['kl_loss']\n",
    "    rv['kl_loss']=kl_loss\n",
    "\n",
    "    # Different losses for different trianing modes.\n",
    "    if mode == 'vae':\n",
    "        # Reconstruction loss\n",
    "        rc_loss = mse(x, x_r)\n",
    "        if 'rc_loss' in scales.keys(): rc_loss *= scales['rc_loss']\n",
    "        rv['rc_loss']=rc_loss\n",
    "        # Average over mini-batch and balance the losses\n",
    "        total_loss = tf.reduce_mean(rc_loss + kl_loss)\n",
    "\n",
    "    if mode == 'dfc':\n",
    "        # get deep features\n",
    "        outputs = model.get_features(x)\n",
    "        outputs_r = model.get_features(x_r)\n",
    "        # Perceptual loss\n",
    "        perceptual_losses = [mse(original, reconstructed) for original, reconstructed in zip(outputs, outputs_r)]\n",
    "        for layer, loss in zip(model.selected_layers, perceptual_losses):\n",
    "            if layer in scales.keys(): loss*=scales[layer]\n",
    "            rv[layer]=loss\n",
    "        percep_loss = sum([rv[layer] for layer in model.selected_layers])\n",
    "        if 'percep_loss' in scales.keys(): percep_loss *= scales['percep_loss']\n",
    "        rv['percep_loss']=percep_loss\n",
    "        total_loss = tf.reduce_mean(percep_loss + kl_loss)\n",
    "\n",
    "    if mode == 'combo':\n",
    "        outputs = model.get_features(x)\n",
    "        outputs_r = model.get_features(x_r)\n",
    "        perceptual_losses = [mse(original, reconstructed) for original, reconstructed in zip(outputs, outputs_r)]\n",
    "        for layer, loss in zip(model.selected_layers, perceptual_losses):\n",
    "            if layer in scales.keys(): loss*=scales[layer]\n",
    "            rv[layer]=loss\n",
    "        percep_loss = sum(perceptual_losses)\n",
    "        if 'percep_loss' in scales.keys(): percep_loss *= scales['percep_loss']\n",
    "        rv['percep_loss']=percep_loss\n",
    "        rc_loss = mse(x, x_r)\n",
    "        if 'rc_loss' in scales.keys(): rc_loss *= scales['rc_loss']\n",
    "        rv['rc_loss']=rc_loss\n",
    "        total_loss = tf.reduce_mean(percep_loss + rc_loss + kl_loss)\n",
    "\n",
    "    rv['total_loss']=total_loss\n",
    "\n",
    "    if test:\n",
    "        rv['x']=x\n",
    "        rv['x_r']=x_r\n",
    "    return rv\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch, model, optimizer, mode, scales):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        loss_dict = compute_loss(model, batch, mode, scales)\n",
    "    inf_gradients=tape.gradient(loss_dict['total_loss'], model.inference_net.trainable_variables)\n",
    "    gen_gradients=tape.gradient(loss_dict['total_loss'], model.generative_net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(inf_gradients, model.inference_net.trainable_variables))\n",
    "    opt2.apply_gradients(zip(gen_gradients, model.generative_net.trainable_variables))\n",
    "    if mode == 'dfc' or mode == 'combo':\n",
    "        opt3.apply_gradients(zip(inf_gradients, model.percep_net.trainable_variables))\n",
    "    return loss_dict\n",
    "\n",
    "# Use a class to create tf.variables on call for AutoGraph\n",
    "class test:\n",
    "    def __init__(self, loss_dict, image_size):\n",
    "        #testing metrics\n",
    "        self.metric_dict = {key: tf.metrics.Mean() for key in loss_dict}\n",
    "        self.losses_dict = loss_dict\n",
    "        self.losses_dict['x']=tf.zeros(shape=(loss_dict['kl_loss'].shape[0], image_size, image_size, 3))\n",
    "        self.losses_dict['x_r']=tf.zeros(shape=(loss_dict['kl_loss'].shape[0], image_size, image_size, 3))\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, model, test_set, step, mode, scales):\n",
    "        with tf.device('/gpu:0'):\n",
    "            for batch in test_set:\n",
    "                self.losses_dict = compute_loss(model, batch, mode, scales, test=True)\n",
    "                for loss, metric in self.metric_dict.items():\n",
    "                    metric.update_state(self.losses_dict[loss])\n",
    "        rv = self.metric_dict['total_loss'].result()\n",
    "        for loss, metric in self.metric_dict.items():\n",
    "            tf.summary.scalar(loss, metric.result(), step=step)\n",
    "            metric.reset_states()\n",
    "        with tf.device('/cpu:0'):\n",
    "            tf.summary.image('input', self.losses_dict['x'], step = step, max_outputs=3)\n",
    "            tf.summary.image('output', self.losses_dict['x_r'], step = step, max_outputs=3)\n",
    "        return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########  Parameters  ############\n",
    "#folder to save weights and images\n",
    "DIR = '../test2'\n",
    "BATCH_SIZE = 32\n",
    "image_size = 192\n",
    "epochs = 1\n",
    "latent_dim = 50\n",
    "optimizer = tf.optimizers.Adam(1e-4)\n",
    "opt2 = tf.optimizers.Adam(1e-4)\n",
    "opt3 = tf.optimizers.Adam(1e-4)\n",
    "log_freq = 10\n",
    "kernelsize = 3\n",
    "mode = 'dfc'\n",
    "model = VAE(latent_dim, image_size, mode)\n",
    "scales = {}\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir='../Documents/img_align_celeba'\n",
    "\n",
    "all_image_paths=get_paths(image_dir)\n",
    "\n",
    "train_paths=all_image_paths[:6400]\n",
    "test_paths=all_image_paths[-640:]\n",
    "#train set defined in the loop for shuffling\n",
    "test_set=from_path_to_tensor(test_paths, BATCH_SIZE, size=image_size)\n",
    "train_dir='./{}/train'.format(DIR)\n",
    "test_dir='./{}/test'.format(DIR)\n",
    "# check if I'm about to overwrite event files\n",
    "train_exists = os.path.exists(train_dir) and len(os.listdir(train_dir))!=0\n",
    "test_exists = os.path.exists(test_dir) and len(os.listdir(test_dir))!=0\n",
    "assert (not train_exists), \"You are going to overwrite your train event files.\"\n",
    "assert (not test_exists), \"You are going to overwrite your test event files.\"\n",
    "# Tensorboard logdirs\n",
    "train_summary_writer = tf.summary.create_file_writer(train_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(1,epochs+1):\n",
    "    train_set= from_path_to_tensor(train_paths, BATCH_SIZE, size=image_size)\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(train_set):\n",
    "        loss_dict = train_step(batch, model, optimizer, mode, scales)\n",
    "        if i==0:\n",
    "            metrics_dict = {key: tf.metrics.Mean() for key in loss_dict}\n",
    "        for loss, value in loss_dict.items():\n",
    "            metrics_dict[loss].update_state(value)\n",
    "        if tf.equal(optimizer.iterations % log_freq, 0):\n",
    "            print('log', optimizer.iterations.numpy())\n",
    "            \n",
    "        if tf.equal(optimizer.iterations % 100, 0):\n",
    "            with test_summary_writer.as_default():\n",
    "                tester = test(loss_dict, image_size)\n",
    "                avg_loss = tester(model, test_set, optimizer.iterations, mode, scales)\n",
    "                print('Epoch: {}, test set average loss: {:.4f},'.format(epoch, avg_loss),\n",
    "                    'time elapsed for current epoch: {:.2f}'.format((time.time() - start_time)/60), 'minutes')\n",
    "            model.saver(DIR, '19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

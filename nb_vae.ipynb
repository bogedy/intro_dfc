{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tfe = tf.contrib.eager\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "#import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "import pathlib\n",
    "AUTOTUNE=tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir=pathlib.Path.cwd()/'../Documents/img_align_celeba/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_paths=list(dir.glob('*'))\n",
    "all_image_paths = [str(path) for path in all_image_paths]\n",
    "image_count = len(all_image_paths)\n",
    "image_count\n",
    "\n",
    "#train_paths=all_image_paths[:-20000]\n",
    "train_paths=all_image_paths[:6400]\n",
    "test_paths=all_image_paths[-320:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/isaiahk/intro_dfc/../Documents/img_align_celeba/008239.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.resize(image, [192, 192])\n",
    "  image /= 255.0  # normalize to [0,1] range\n",
    "  #image = tf.image.convert_image_dtype(image, tf.float16)\n",
    "\n",
    "  return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "  image = tf.io.read_file(path)\n",
    "  return preprocess_image(image)\n",
    "\n",
    "\n",
    "def from_path_to_tensor(paths, batch_size):\n",
    "    path_ds=tf.data.Dataset.from_tensor_slices(paths)\n",
    "    ds=path_ds.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds=ds.batch(batch_size)\n",
    "    ds=ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE=image_count//9\n",
    "\n",
    "train_set= from_path_to_tensor(train_paths, BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "test_set=from_path_to_tensor(test_paths, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(CVAE, self).__init__()\n",
    "    self.latent_dim = latent_dim\n",
    "    self.inference_net = tf.keras.Sequential(\n",
    "      [\n",
    "          tf.keras.layers.InputLayer(input_shape=(192, 192, 3)),\n",
    "          tf.keras.layers.Conv2D(\n",
    "              filters=8, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "          tf.keras.layers.Conv2D(\n",
    "              filters=4, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "          tf.keras.layers.Conv2D(\n",
    "              filters=2, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "          tf.keras.layers.Flatten(),\n",
    "          # No activation\n",
    "          tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.generative_net = tf.keras.Sequential(\n",
    "        [\n",
    "          tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "          tf.keras.layers.Dense(units=24*24*32, activation=tf.nn.relu),\n",
    "          tf.keras.layers.Reshape(target_shape=(24, 24, 32)),\n",
    "          tf.keras.layers.Conv2DTranspose(\n",
    "              filters=2,\n",
    "              kernel_size=3,\n",
    "              strides=(2, 2),\n",
    "              padding=\"SAME\",\n",
    "              activation='relu'),\n",
    "          tf.keras.layers.Conv2DTranspose(\n",
    "              filters=4,\n",
    "              kernel_size=3,\n",
    "              strides=(2, 2),\n",
    "              padding=\"SAME\",\n",
    "              activation='relu'),\n",
    "          tf.keras.layers.Conv2DTranspose(\n",
    "              filters=8,\n",
    "              kernel_size=3,\n",
    "              strides=(2, 2),\n",
    "              padding=\"SAME\",\n",
    "              activation='relu'),  \n",
    "          # No activation\n",
    "          tf.keras.layers.Conv2DTranspose(\n",
    "              filters=3, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation='sigmoid'),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  def sample(self, eps=None):\n",
    "    if eps is None:\n",
    "      eps = tf.random.normal(shape=(4, self.latent_dim))\n",
    "    return self.decode(eps)\n",
    "  @tf.function\n",
    "  def encode(self, x):\n",
    "    mean, logvar = tf.split(self.inference_net(x), num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "  def reparameterize(self, mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "  @tf.function\n",
    "  def decode(self, z):\n",
    "    return self.generative_net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "  log2pi = tf.math.log(2. * np.pi)\n",
    "  return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "def compute_loss(model, x, test=False):\n",
    "    mean, logvar = model.encode(x)\n",
    "    z = model.reparameterize(mean, logvar)\n",
    "    x_r = model.decode(z)\n",
    "\n",
    "    rc_loss = K.sum(K.binary_crossentropy(\n",
    "    K.batch_flatten(x), \n",
    "    K.batch_flatten(x_r)), axis=-1)\n",
    "\n",
    "    # Regularization term (KL divergence)\n",
    "    kl_loss = -0.5 * K.sum(1 + logvar \\\n",
    "                             - K.square(mean) \\\n",
    "                             - K.exp(logvar), axis=-1)\n",
    "    \n",
    "    # Average over mini-batch\n",
    "    total_loss = K.mean(rc_loss + kl_loss)\n",
    "    \n",
    "    if test:\n",
    "        return rc_loss, kl_loss, total_loss, x, x_r\n",
    "    else:\n",
    "        return rc_loss, kl_loss, total_loss\n",
    "\n",
    "\n",
    "def compute_gradients(model, x):\n",
    "  with tf.GradientTape() as tape:\n",
    "    _, _2, loss = compute_loss(model, x)\n",
    "  return tape.gradient(loss, model.trainable_variables), loss\n",
    "\n",
    "def apply_gradients(optimizer, gradients, variables):\n",
    "     optimizer.apply_gradients(zip(gradients, variables))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "latent_dim = 50\n",
    "num_examples_to_generate = 4\n",
    "\n",
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement.\n",
    "random_vector_for_generation = tf.random.normal(\n",
    "    shape=[num_examples_to_generate, latent_dim])\n",
    "model = CVAE(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  predictions = model.sample(test_input)\n",
    "  fig = plt.figure(figsize=(2,2))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(2, 2, i+1)\n",
    "      plt.imshow(predictions[i, :, :, :])\n",
    "      plt.axis('off')\n",
    "\n",
    "  # tight_layout minimizes the overlap between 2 sub-plots\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(metrics, tags, test=False):\n",
    "    if test:\n",
    "        with test_summary_writer.as_default():\n",
    "            for metric, tag in zip(metrics, tags):\n",
    "                tf.summary.scalar(tag, metric.result(), step=optimizer.iterations)\n",
    "                metric.reset_states()\n",
    "    else:\n",
    "        with train_summary_writer.as_default():\n",
    "            for metric, tag in zip(metrics, tags):\n",
    "                tf.summary.scalar(tag, metric.result(), step=optimizer.iterations)\n",
    "                metric.reset_states()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(batch, model, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        rc_loss, kl_loss, loss = compute_loss(model, batch)\n",
    "    gradients=tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return rc_loss, kl_loss, loss\n",
    "\n",
    "\n",
    "losses=['loss','rc_loss','kl_loss']\n",
    "\n",
    "def train_vae(model, optimizer, epochs, dataset, print_interval, save_interval, log_freq=10):\n",
    "    #summary_writer = tf.summary.create_file_writer(DIR)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        avg_loss=tf.metrics.Mean(name='loss', dtype=tf.float32)\n",
    "        avg_val=tf.metrics.Mean(name='val_loss', dtype=tf.float32)\n",
    "        avg_train_rc=tf.metrics.Mean(name='training_rc_loss', dtype=tf.float32)\n",
    "        avg_val_rc=tf.metrics.Mean(name='val_rc_loss', dtype=tf.float32)\n",
    "        avg_train_kl=tf.metrics.Mean(name='training_kl_loss', dtype=tf.float32)\n",
    "        avg_val_kl=tf.metrics.Mean(name='val_kl_loss', dtype=tf.float32)\n",
    "        start_time = time.time()\n",
    "        for step, batch in enumerate(dataset):\n",
    "            rc_loss_x, kl_loss_x, loss_x = train_step(batch, model, optimizer)\n",
    "            avg_loss.update_state(loss_x)\n",
    "            avg_train_rc.update_state(rc_loss_x)\n",
    "            avg_train_kl.update_state(kl_loss_x)\n",
    "            if tf.equal(optimizer.iterations % log_freq, 0):\n",
    "                print('log', '{}'.format(optimizer.iterations.numpy()))\n",
    "                summarize([avg_loss, avg_train_rc, avg_train_kl], losses)\n",
    "            if (time.time()-start_time)>5:\n",
    "                print('time to check!')\n",
    "                for batch in test_set:\n",
    "                    print('do we get here?')\n",
    "                    rc_val, kl_val, val, x, x_r=compute_loss(model, batch, test=True)\n",
    "                    avg_val.update_state(val)\n",
    "                    avg_val_rc.update_state(rc_val)\n",
    "                    avg_val_kl.update_state(kl_val)\n",
    "                print('Batch',step,'done.', 'loss on validaiton set: {}'.format(avg_val.result()))\n",
    "                summarize([avg_val,avg_val_rc, avg_val_kl], losses,test=True)\n",
    "                with test_summary_writer.as_default():\n",
    "                    tf.summary.image('input', x, step = optimizer.iterations, max_outputs=3)\n",
    "                    tf.summary.image('output', x_r, step = optimizer.iterations, max_outputs=3)\n",
    "                start_time=time.time()\n",
    "            if step+1 % save_interval ==0:\n",
    "                generate_and_save_images(model, epoch, step+1, random_vector_for_generation)\n",
    "                model.weight_saver(TRAINING_DIR, epoch, i)\n",
    "                end_time = time.time()\n",
    "\n",
    "        '''if epoch % 1 == 0:\n",
    "            loss = tf.zeros(20000//BATCH_SIZE+1)\n",
    "            j=0\n",
    "            for test_x in test_set:\n",
    "                loss[j]=compute_loss(model, test_x)\n",
    "                j+=1\n",
    "                elbo = -np.mean(loss)\n",
    "            #display.clear_output(wait=False)\n",
    "            print('Epoch: {}, Test set ELBO: {},'.format(epoch, elbo),\n",
    "                'time elapse for current epoch {}'.format(epoch,elbo,end_time - start_time))\n",
    "'''\n",
    "    #generate_and_save_images(model, epoch,0, random_vector_for_generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=tf.optimizers.Adadelta(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DIR='train1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary_writer = tf.summary.create_file_writer(TRAINING_DIR+'/summaries/train')\n",
    "test_summary_writer = tf.summary.create_file_writer(TRAINING_DIR+'/summaries/test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0614 16:51:56.848995 140176657299200 deprecation.py:323] From /home/isaiahk/miniconda3/envs/tf2b/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log 10\n",
      "log 20\n",
      "log 30\n",
      "log 40\n",
      "time to check!\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "Batch 43 done. loss on validaiton set: 76650.96875\n",
      "log 50\n",
      "log 60\n",
      "log 70\n",
      "log 80\n",
      "time to check!\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "Batch 85 done. loss on validaiton set: 76650.7890625\n",
      "log 90\n",
      "log 100\n",
      "log 110\n",
      "time to check!\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "Batch 116 done. loss on validaiton set: 76650.734375\n",
      "log 120\n",
      "log 130\n",
      "log 140\n",
      "time to check!\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "Batch 141 done. loss on validaiton set: 76650.8125\n",
      "log 150\n",
      "log 160\n",
      "time to check!\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "Batch 163 done. loss on validaiton set: 76650.6171875\n",
      "log 170\n",
      "log 180\n",
      "time to check!\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "do we get here?\n",
      "Batch 183 done. loss on validaiton set: 76650.484375\n",
      "log 190\n",
      "log 200\n"
     ]
    }
   ],
   "source": [
    "train_vae(model, optimizer, 1, train_set, 10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2b] *",
   "language": "python",
   "name": "conda-env-tf2b-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
